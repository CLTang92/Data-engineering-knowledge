Build and automate a python ETL process that would extract real estate properties data from an API, 
loads it unto amazon s3 bucket which then triggers a series of lambda functions which then ultimately transforms the data, 
converts into a csv file format and load the data into another S3 bucket using Apache Airflow.

Apache airflow will utilize an S3KeySensor operator to monitor if the transformed data has been uploaded into the aws S3 bucket before attempting to load the data into an amazon redshift. 
After the data is loaded into aws redshift, then will connect amazon quicksight to the redshift cluster to then visualize the data.


To learn
- Use of AWS cloud platform.
- Install Apache airflow from scratch and schedule ETL pipeline
- How to use sensor in ETL pipeline.
- How to setup aws lambda function from scratch, set up aws redshift and aws quicksight.


API -> Python Extract data, load to ->S3 Bucket (landing zone), trigger-> lambda function, copy/load -> S3 bucket (Intermediate zone), trigger 
-> lambda function, tranform/load -> s3 bucket (Transform data), load -> Redshift -> QuickSight

* using Apache Airflow as orchestration tool and running in EC2 instance


DAG to build
tsk_extract_data_var (PythonOperator) - connecting to api, extract data and adding data to EC2->tsk_load_to_s3 (BashOperator)- move data from EC2 to s3-> tsk_is_file_in_s3_available (S3KeySensor), sense transformed data in s3 bucket->tsk_transfer_s3_redshift(S3ToRedshiftOperator), transfer data from s3 to redshift

-----------------------------------
##Create user group in AWS
- Go to AWS.com -> sign in
**make sure Oregon is your location
- create user group (not using root user, assigning user to group), assign privileges
eg. read and write group
-IAM - User groups (Access Management) - click create group - user group name: endtoendendusergroup - add users to the group (empty as for now) -> attach permissions policies - AdministratorAccesss -> create group

#create user
Users (Access Management) - Create User - username: endtoenduser1 - provide user access to AWS Management Console (check on I want to create an IAM user) - set Custom password, uncheck users must create a new password at next sign-in-next
-Add user to group - select user groups - next - create user Download .csv file (there is a console sign-in URL) - return to user list
- click the user that just created - go to Security credentials - under Access keys - create access keyv- select command line interface (CLI)
- Check on I understand the above recommendation and want to proceed to create an access key. - Next - Create access key - **copy Access key and secret access key OR Download .csv file - Done

#logout from root user, now login with new user with console sign-in URL (in csv file)


#sign in as IAM user
account id is the number after https:// in console sign-in URL (in csv file)

#sign in using console sign-in URL (in csv file), put username and password from csv file

After sign in as IAM user
--------------------------
##Create EC2 instance **Make sure at correct Oregon
- search EC2 - Launch Instance (going to install Airflow) - Name: endtoendEC2 - Amazon Machines image: Ubuntu - Amazon Machine Image: Free tier eligible - Instance type: for Airflow can't use t2.micro, can use t2.small (maybe lag, can use t2.medium)
- Key pair(login)-Select key pair OR create new key pair, will give a .pem file and save in any location
- Under Firewall (security groups), create security group, check on allow SSH traffic from Anywhere, Allow HTTPS traffic from the internet (total 3 checkboxes) - Launch instance
- Go to EC2/Instances to see the new instance created, refresh the page, the instance state should be Running

#install dependency
click the EC2 instance - click top right "Connect" - Under "EC2 Instance Connect" tab, connect using EC2 Instance Connect - Connect - A terminal is open

=====in terminal install dependency=========
#make update to EC2
sudo apt update

#install pip
sudo apt install python3-pip
*press y to continue

*Deamons using outdated libraries, which services should be restarted? - choose networkd-dispatcher.service and enter
** if the progress show 99%, actually should be done


- go back just now the same steps to connect another terminal by clicking connect

======in terminal==================
#virtual environment
sudo apt install python3.10-venv
*press y to continue
* Deamons using outdated libraries, which service should be restarted? select networkid-dispatcher.service, then enter

#create virtual environment, eg. endtoend_venv is the name of virtual environment
python3 -m venv endtoend_venv


#activate virtual environment
source endtoend_venv/bin/activate


#install aws cli
pip install --upgrade awscli

#install Airflow
sudo pip install apache-airflow

#initiate Apache Airflow
airflow standalone

==============================

Go to Airflow UI
1) Go back to EC2 page (Under EC2/Instances), check on the EC2 Instance, under Details tab, there is a Public IPv4 address OR Public IPv4 DNS, copy either one
2) paste the public ip address to a new browser tab and at the end of the url type :8080 *this is the port that Airflow is going to run
3) when hit enter in browser, Airflow is not connected because haven't open the port for Airflow to run on EC2, so back to EC2 page (Under EC2/Instances), check on the EC2 Instance, under Security tab, look for Security groups, click on the group,
under "Inbound rules" - select right button "Edit inbound rules", scroll down, click "Add rule", 
Type: Custom TCP, Protocol: TCP, Port range: 8080, Source: Anywhere IPv4 - save rules *if source select my ip, only your ip can access airflow
4) reload again the url, should be connected to Airflow, login credential can get it during initializing Airflow just now (terminal) and sign in

###start developing at visual studio code######
1) connect visual studio code to EC2 instance *anoher video, how to remotely SSH (connect) visual studio code to aws ec2
2) after connected, go to visual studio code, select File - Open folder

3) under airflow folder - airflow.cfg
dag_folder = /home/ubuntu/airflow/dags *location of dag, airflow pipelie live, python file

load_examples = True (this means at Airflow UI, there are showing examples of DAGs, to remove, set to False and save)

***update anything at airflow.cfg, need to restart Airflow server at terminal (ctrl C at terminal to shut down server, then type airflow standalone to restart Airflow)

*in this case, create dags folder if not exists, after created the folder, create py file, can name it as analytics.py

4) In analytics.py file (all DAGS put here)
5) To get data, Go to Rapid Api - web browser, https://rapidapi.com/hub/, create account and login
- Go to "Search for APIs" search bar and type "Zillow", under pricing will show details about pricing, then select Endpoints, click on "Subscribe to Test"
- Go to the left panel, under "Properties", select api we want eg. "Search for properties"
- Go to right panel, under "Code Snippets", click on (Node.js)Axios drop down list, select Python- Requests (because want to use Python),
then it will show the code snippets, can test it by clicking "TestEndpoint" button and look at Results tab or refer "Example Responses"


#########STEP 1: RETRIEVE DATA FROM API: DAG (tsk_extract_data_var)############################

**Under analytics.py file

from airflow import DAG #to recognize this python is DAG file 
from datetime import timedelta, datetime
import json
import requests
from airflow.operators.python import PythonOperator



# Load JSON config file
with open('/home/ubuntu/airflow/config_api.json', 'r') as config_file: #r means read, open the config file, remember to import json
    api_host_key = json.load(config_file)


now = datetime.now()
dt_now_string = now.strftime("%d%m%Y%H%M%S")

def extract_zillow_data(**kwargs): #kwargs means keyword args, can refer to the airflow documentation
    url = kwargs['url']
    headers = kwargs['headers']
    querystring = kwargs['querystring']
    dt_string = kwargs['date_string']
    # return headers
    response = requests.get(url, headers=headers, params=querystring) #same as rapid api
    response_data = response.json()
    


    # Specify the output file path
    output_file_path = f"/home/ubuntu/response_data_{dt_string}.json" #st_string filename in date format
    file_str = f'response_data_{dt_string}.csv' #this is for lambda later

    # Write the JSON response to a file
    with open(output_file_path, "w") as output_file: #write data to output_file
        json.dump(response_data, output_file, indent=4)  # indent for pretty formatting #is going to save inside Ubuntu EC2
    output_list = [output_file_path, file_str] #this list is used for later to access it, can access later by indexing 0, 1
    return output_list   

default_args = {
    'owner': 'airflow',
    'depends_on_past': False, #don't want the past
    'start_date': datetime(2023, 8, 1), #get data since when
    'email': ['myemail@domain.com'], #any email
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(seconds=15) #wait 15 seconds before 2nd retry
}


*At Rapid Api, under code snippets, there is a headers field with API Key and API Host, 
don't want to expose these information in our code, so we can create a new json file under Airflow folder 
and name the file as config_api.json
- in this config_api.json, copy the "headers" field API key and host and paste them here, remove header word, remain 
{
	"X-RapidAPI-Key": "xxxxxxxxxxx",
	"X-RapidAPI-Host": "xxxxxxxxxx"
}



with DAG('zillow_analytics_dag', #dag id we give
        default_args=default_args,
        schedule_interval = '@daily', #run every 12 mignight
        catchup=False) as dag:

        extract_zillow_data_var = PythonOperator( #import PythonOperator #import json as the we don't want to expose the api key
        task_id= 'tsk_extract_zillow_data_var', #any name
        python_callable=extract_zillow_data, #python callable can get from documentation,under airflow.operators.python, extract_zillow_data is a function name we give
        op_kwargs={'url': 'https://zillow56.p.rapidapi.com/search', 'querystring': {"location":"houston, tx"}, 'headers': api_host_key, 'date_string':dt_now_string}
	#the parameter with used refer to the response paramater required by the API, add in additional date_string because want to use it to rename the filename according to date
        #op_kwargs is a dictionary of keyword arguments that will get unpacked in your function, refer to Airflow documentation
	#all the values in op_kwargs which are url, headers, querystring and date_string will pass to the above extract_zillow_data function
	)

- Save
- Refresh Airflow UI and see whether the DAG is shown
** maybe need to wait for 2 to 5 minutes to see the DAG created
- once the dag is shown, click on the DAG link, go to Graph view, can see 1 task there, at right side, Layout selected is Left -> Right
- code tab will all the code we wrote
- this DAG is for extracting data from Rapid api
- above color box represent status, queue means data in queue. these boxes are called legend
- try to run DAG at Airflow UI by clicking "Trigger" icon button at right side, after that check whether there are json files extracted in the visual studio code EC2
* if there is yellow means retrying, can check the log tab to know any error to debug
* if failed, to rerun, click on the task and click "Clear task", it will auto rerun again
- can double click the extracted json file to see the data

##################STEP 2: MOVE THE EXTRACTED DATA TO S3 BUCKET#################

- once the data (json file) located at EC2, use BashOperator to move the file to S3

Steps:
1) create S3 bucket
- Go to AWS, search S3, click on "Create bucket"
Bucket name: endtoendbucket **must be unique
AWS Region: <choose your region> **EC2 and S3 must be same region
- create bucket


2) start create 2nd task: BashOperator

from airflow.operators.bash_operator import BashOperator


   	 load_to_s3 = BashOperator(
            task_id = 'tsk_load_to_s3', #task id any name
            bash_command = 'aws s3 mv {{ ti.xcom_pull("tsk_extract_zillow_data_var")[0]}} s3://endtoendbucket/', #need to install aws dependency, mv means move, so here use aws command to move data from output_file_path to S3
            #ti means task in, allow to connect whatever previous task to get something from, xcom means cross communication, ti.xcom_pull can pull data from previous task, "tsk_extract_zillow_data_var" is the task id to pull from which is output_list, [0] means extract first thing "output_file_path"
        )



3) create role to allow EC2 instance to access S3 bucket
- select IAM
- At left panel, under "Access Management" - Roles
- At the right - select "Create Role" #to assign to EC2
Trusted entity type: AWS SERVICE
Use case: EC2
- click next
- Permissions policies: AmazonS3FullAccess
- Click Next
- Role name:ec2SeveralAccess-08232023
- Description: Allows EC2 instances to call AWS servuces on your behalf. 
- Click Create Role

4) Modify IAM role for EC2 instance
- Got to EC2 instance, check on the "endtoendEC2" instance, click top right "Action" button - "Security" - "Modify IAM role"
- select the new created IAM role
- Click on "Update IAM role"


5) To show dependency in Airflow UI

At visual studio code:

extract_zillow_data_var >> load_to_s3 

6) test run in Airflow
***before that delete the json file created just now and go to Airflow UI, select the DAG and go to graph view and trigger the DAG by clicking "Trigger" icon at right side

##UNTIL HERE DONE EXTRACTING DATA FROM API AND SAVE TO EC2, THEN MOVE DATA FROM EC2 TO S3 BUCKET###########



#create LAMBDA
1) In AWS, search Lambda
2) Click on "Create function"
3) select Author from scratch #select this as we create from scratch
4) Function name: copyRawJsonFile-lambdaFunction #any name
5) Runtime: Python 3.10 #language use
6) Architecture x86_64
7) Change default execution role: Use an existing role, select lambdaFunctionfullS3access-cloudwatch **go and create IAM before selecting this, follow steps below
#go to IAM - Roles - Create role - trusted entity type: AWS service, Use case: Lambda - Next - Add 2 permissions: AmazonS3FullAccess and AWSLambdaBasicExecutionRole (this permission provide write permissions to CloudWatch Logs) - Next -
Role name: lambdaFunctionfullS3access-cloudwatch - click "Create role"
8) click "Create function"



##add triggers in Lambda function, whenever there is an event (load json file) in bucket (landing zone) will trigger lambda function
1) under Function overview of the lambda function "copyRawJsonFile-lambdaFunction", click on "Add trigger"
2) Source: S3
Bucket: endtoendbucket #whenever event in this bucket will trigger this lambda function
Event types: All object create events

#Prefix eg. images/ refers to folder, whenever an event happen in images folder of this bucket, trigger the lambda function
#Suffix eg. .jpg refers to the file type, whenever receive this file type in this bucket, trigger the lambda function

3) Recursive invocation
Click on "I acknowledged that using the same s3 bucket for both input and output is not recommended..."
#in this case different S3 buckets for input and output, is fine
4) Click "Add"
5) Go to "Code" tab of the lambda function, use the codes below.

import boto3 #by default already install boto3
import json

s3_client = boto3.client('s3') #call S3 client

def lambda_handler(event, context): #the event in this example is the file entered
    # TODO implement
    source_bucket = event['Records'][0]['s3']['bucket']['name'] #to get this event['Records'][0]['s3']['bucket']['name'], can click on "Test" button, under "Template-optional" select "s3-put", can refer to Event JSON sample, [0] is the first list item
    object_key = event['Records'][0]['s3']['object']['key'] #key is file name enter into S3 bucket
   
    
    target_bucket = 'copy-of-raw-json-bucket'
    copy_source = {'Bucket': source_bucket, 'Key': object_key}
   
    waiter = s3_client.get_waiter('object_exists')
    waiter.wait(Bucket=source_bucket, Key=object_key) #wait for the object exists, because sometimes the file is very big, wait for the file fully loaded
    s3_client.copy_object(Bucket=target_bucket, Key=object_key, CopySource=copy_source) #Key=object_key giving the same filename name as previous filename
    return {
        'statusCode': 200,
        'body': json.dumps('Copy completed successfully')
    }




6) click "Deploy" button to save
**to see output, can go to "Monitor" tab - View CloudWatch logs (**already given this lambda access to CloudWatch), click on the first log stream to see log events


7) now create the S3 bucket that we want to put the file into
- search s3 - Buckets - Create bucket - Bucket name: copy-of-raw-json-bucket, make sure correct region - create bucket

8) can run Airflow again to see result


########################UNTIL HERE CAN COPY FILE FROM THE ORIGIN S3 BUCKET TO ANOTHER S3 BUCKET (INTERMEDIATE) BY AUTOMATICALLY TRIGGER A LAMBDA FUNCTION WHEN FILE LOADED IN ORIGIN S3 BUCKET################


############STEP 3: CREATE ANOTHER LAMBDA FUNCTION TO TRIGGER INTERMEDIATE BUCKET TO DO TRANSFORMATION AND LOAD FILE TO ANOTHER BUCKET, THEN CREATE S3KeySensor TO SENSE WHETHER THE CSV FILE IN THE BUCKET ###################3

#create another lambda
1) In AWS, search Lambda
2) Click on "Create function"
3) select Author from scratch #select this as we create from scratch
4) Function name: transformation-convert-to-csv-lambdaFunction #any name
5) Runtime: Python 3.10 #language use
6) Architecture x86_64
7) Change default execution role: Use an existing role, select lambdaFunctionfullS3access-cloudwatch **Earlier already created the role when creating earlier lambda function
8) click "Create function"



##add triggers in Lambda function, when file copy into bucket (intermediate bucket) will trigger lambda function to do transformation and load to another bucket
1) under Function overview of the lambda function "transformation-convert-to-csv-lambdaFunction", click on "Add trigger"
2) Source: S3
Bucket: copy-of-raw-json-bucket #whenever event in this bucket will trigger this lambda function
Event types: All object create events

3) Recursive invocation
Click on "I acknowledged that using the same s3 bucket for both input and output is not recommended..."
#in this case different S3 buckets for input and output, is fine
4) Click "Add"
5) Go to "Code" tab of the lambda function, use the codes below.

##transformations below are convert filetype from json to csv and just select some columns only
import boto3
import json
import pandas as pd

s3_client = boto3.client('s3')

def lambda_handler(event, context):
    # TODO implement
    source_bucket = event['Records'][0]['s3']['bucket']['name']
    object_key = event['Records'][0]['s3']['object']['key']
    
    target_bucket = 'cleaned-data-zone-csv-bucket'
    target_file_name = object_key[:-5] #getting filename from the beginning and ignore the last 5 characters, in the case filename is xxxxxxxxxx.json, so it will exclude .json since it is the last 5 characters #reason is want to use the same name but append .csv as suffix
    #print(target_file_name) #result can see in cloudwatch
   
    waiter = s3_client.get_waiter('object_exists')
    waiter.wait(Bucket=source_bucket, Key=object_key)
    
    response = s3_client.get_object(Bucket=source_bucket, Key=object_key)
    #print(response)
    data = response['Body']
    #print(data)
    data = response['Body'].read().decode('utf-8')
    #print(data)
    data = json.loads(data)
    print(data)
    f = []
    for i in data["results"]: #at json file there, there are a list of dictionaries under "results", each dictionaries represent a record
        f.append(i)
    df = pd.DataFrame(f) #converting json to dataframe
    # Select specific columns
    selected_columns = ['bathrooms', 'bedrooms', 'city', 'homeStatus', 
                    'homeType','livingArea','price', 'rentZestimate','zipcode']
    df = df[selected_columns] #now the dataframe only with selected column
    print(df)
    
    # Convert DataFrame to CSV format
    csv_data = df.to_csv(index=False) #index=False, don't want the index
    
    # Upload CSV to S3
    bucket_name = target_bucket
    object_key = f"{target_file_name}.csv"
    s3_client.put_object(Bucket=bucket_name, Key=object_key, Body=csv_data)
    
    
    return {
        'statusCode': 200,
        'body': json.dumps('CSV conversion and S3 upload completed successfully')
    }



6) click "Deploy" to save
7) Add a layers #to install pandas for this code
- at the lambda function, scroll down and go to Layers - Add a layer
Runtime : Python 3.10
AWS layers: AWSSDKPandas-Python310
Version: 4 #any version
- Add

8) now create the S3 bucket to save transformed data later ##can do before putting code in lambda step 5
- search s3 - Buckets - Create bucket - Bucket name: cleaned-data-zone-csv-bucket, make sure correct region - create bucket

9) can run Airflow again to see result


#create S3KeySensor to sense whether csv file is available before trigger S3ToRedshiftOperator
---------in visual studio code---------
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor 

# Define the S3 bucket
s3_bucket = 'cleaned-data-zone-csv-bucket'

        is_file_in_s3_available = S3KeySensor(
        task_id='tsk_is_file_in_s3_available',
        bucket_key='{{ti.xcom_pull("tsk_extract_zillow_data_var")[1]}}',
        bucket_name=s3_bucket,
        aws_conn_id='aws_s3_conn',
        wildcard_match=False,  # Set this to True if you want to use wildcards in the prefix
        timeout=60,  # Optional: Timeout for the sensor (in seconds)
        poke_interval=5,  # Optional: Time interval between S3 checks (in seconds)
        )

extract_zillow_data_var >> load_to_s3 >> is_file_in_s3_available 


9) ****since import S3KeySensor "from airflow.providers.amazon.aws.sensors.s3", so need to install apache airflow provider amazon
**in Terminal
#activate virtual environment
source endtoend_venv/bin/activate

pip install apache-airflow-providers-amazon #install this, s3aKeySensor only can work



10) restart server at AWS EC2
- CTRL + C #EXIT
- #initiate Apache Airflow to start it again
airflow standalone


11) 
#need to create a connection in Airflow UI to allow Airflow to access S3
- copy aws_con_id under S3KeySensor which is aws_s3_conn in this case
- go to Airflow refresh since just now restarted Airflow
- at Airflow UI, click "Admin" - Connections - click "+"
Connection Id:aws_s3_conn #aws conn id
Connection Type: Amazon Web Services
AWS Access Key ID: # got it when creating AWS user
AWS Secret Access Key: #got it when creating AWS user
- save

12) go back to AWS lambda function, both lambda function set timeout
- under "Configuration" tab - General configurationn - Edit - set Timeout: 5 minutes #in 5 minutes if this lambda didn't do what we want it to do, then will timeout, max is 15 minutes, so if code run more than that can't use or maybe need to change Memory in settings


13) run Airflow DAG again, see in Graph view
- save
* can see the log of task by selecting the task and click logs
* to check any error can look back each bucket

#########UNTIL HERE DONE LOADING TRANSFORMED DATA TO BUCKET AND CREATED A S3KEYSENSOR TO SENSE WHETHER THE CSV FILE IS IN THE BUCKET##############


###########STEP 4 (LAST STEP):  LOAD DATA TO REDSHIFT AND DO VISUALIZATION USING QUICKSIGHT########################

1) In AWS, search Amazon Redshift
2) scroll down, at the right side, click on "Create cluster" under "For more granular control"
Choose the size of the cluster: I'll choose
Node Type: ra3.xlplus #can look at the price
Number of nodes: 1 #number of nodes will affect price
Admin user name: awsuserzillow #any name
Admin user password:  #insert password
*AmazomRedshiftAllCommandsFullAccess already created as default
3) click "Create cluster"
*creating maybe take 1 or 2 minutes
4) go to left side panel, select Clusters - Query editor v2
5) #connect to Redshift cluster 1
- At the left side, click the cluster name (eg. redshift-cluster-1)
- Under authentication, select database user name and password #user name and password created just now
- Database: dev #any name
- User name #the one created just now
- Password #the one created just now
- Click on "Create connection" #create connection so can run code here

* Left panel can see "dev" database, "public" is the public schema
* sample_data_dev is empty because didn't click sample data just now

6) #create table to load data

- in Redshift query editor v2

CREATE TABLE IF NOT EXISTS zillowdata(
bathrooms NUMERIC,
bedrooms NUMERIC,
city VARCHAR(255),
homeStatus VARCHAR(255),
homeType VARCHAR(255),
livingArea NUMERIC,
price NUMERIC,
rentZestimate NUMERIC,
zipcode INT
)

- click run
- Refresh Tables at dev/public/Tables

- SELECT *
FROM zillowdata
*highlight only this statement and run to see the result below, right now no data, but have column name


-------------------------------------------------

#####After created table in Redshift, now go to Airflow and create a task to load data to Redshift table.



#Visual studio code

from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator


transfer_s3_to_redshift = S3ToRedshiftOperator(
        task_id="tsk_transfer_s3_to_redshift",
        aws_conn_id='aws_s3_conn', #give connection to S3
        redshift_conn_id='conn_id_redshift',

        s3_bucket=s3_bucket, #cleaned data csv bucket
        s3_key='{{ti.xcom_pull("tsk_extract_zillow_data_var")[1]}}', #filename that we want to get from the S3 bucket
        schema="PUBLIC",#can go to Redshift there, schema is under public
        table="zillowdata",
        copy_options=["csv IGNOREHEADER 1"], #ignore header in row 1 of csv file


	extract_zillow_data_var >> load_to_s3 >> is_file_in_s3_available >> transfer_s3_to_redshift #dependency
    )



#create Redshift connection on Airflow

1) go to Airflow UI, Admin - Connections - create connection by clicking "+" 
Connection Id: conn_id_redshift
Connection type: Amazon Redshift
Description: (optional, good to put it)
Host: redshift-cluster-1...... amazon.com #under Redshift cluster, click the link, at the right there is an endpoint, copy and paste it here and remove from the back, keep from start until amazon.com
Database: dev #name of the Redshift database
User: #created when creating Redshift
Password: #created when creating Redshift 
Port: 5439
-save

#edit previous role, need to add permission to access redshift
1) go to AWS IAM - Roles - 

ec2SeveralAccess.. - click Add permission (at right side)- Attach policies
2) check on AmazonRedshiftFullAccess - Add permissions #add permission for ec2 to access Redshift, because Airflow is running on EC2

#go to redshift edit inbound rule
1) Go to Amazon Redshift/Clusters/redshift-cluster-1
2) Under "Properties" tab, under "Network and Security Settings" - "VPC Security Group", there is a link click on it, 

click on "Inbound rules", if shows that ipv4 source 0.0.0.0/0, type all traffic, port All should be no problem, allowing all traffic, no change is required.

3) run again DAG in Airflow

4) go to Redshift, run again "SELECT * FROM zillowdata #show all the data in the table

#show how many records
SELECT COUNT(*) FROM zillowdata



####UNTIL HERE DONE LOAD CSV FILE TO REDSHIFT#####


####STEP CONNECT REDSHIFT TO QUICKSIGHT#####
1) At AWS, search QuickSight
2) Click "Sign up for QuickSight" and scroll down click on the "Sign up for standard edition here" #free version
Authentication method: Use IAM federated identities & QuickSight managed users

Select a region: #my region
QuickSight account name: deairflow #any name
Notification email address: #enter email address
Allow access and autorecovery for these resources: Check on Amazon Redshift, other if want can check it
3) Click "Finish"
4) After done, take note the account name, can copy and save at note
5) Click "Go to Amazon QuickSight"
**Go to top right corner, there is an profile icon, click on it and select "Manage QuickSight", over here can invite users (if any), under "Security & permissions" tab, already have Amazon Redshift 
6) On left panel, click on "Dataset", there are some datasets given by AWS, click on "New dataset" on the right side, select Redshift Auto-discovered"

Data source name: zillowdataset
Instance ID: redshift-cluster-1
Connection type: Choose a VPC connection #here no selection
Database name: dev
Username: #Redshift username
Password: #Redshift password

7) Click "Validate connection"
＊if error: Amazon QuickSight can't reach your data source because it's inside a private network. To fix this, make your host publicly accessible. Show details. Click show details, sourceErrorMessage: Unable to route to host address redshift-cluster-1 
**to solve this, go to Amazon Redshift/clusters/redshift-cluster-1, under "Properties tab" - "Network and Security settings", over here "Publicly accessible " is disabled, so click "Edit" button, Publicly accessible: check on "Turn On Publicly accessible". Elastic IP Address: None - 

Save changes, wait for few minutes for modifying
8) At QuickSight, click on "Create data source".
Schema contains sets of tables: public
Tables: contain the data you can visualize: click "zillowdata", then can edit/preview data, then go to top right corner click " Save & Publish"
9) go to top left corner, click "QuickSight" - Datasets, can see zillowdata now
**to delete dataset, click on the zillowdata dataset, can delete data set by clicking "DELETE DATASET" at top right button

#Visualization in QuickSight
1) click on zillowdata (in dataset tab), click "USE IN ANALYSIS" button.
2) can close "New sheet" window
3) field lists are all the columns, can drag columns you want to Autograph , and select visualize types
4) by clicking the "value" button above Autograph (or click on the graph), can drag column to x-axis to set x-axis (eg. Bedroom(sum)) and y-axis (eg. price(sum)) is the value, in value can set y-axis to average price by selecting average. Example average price.

#duplicate graph to modify x-axis to do another graph
1) click vertical 3 dots button of the graph, then select "Duplicate visual"
*can resize the graph or drag the graph to adjust location
2) above the 2nd, click the selected field (eg. Bedroom) in x-axis, and click remove. Then drag and drop zip code column to x-axis field. Now there is a new graph with x-axis zip code.

#COMPLETED#




























